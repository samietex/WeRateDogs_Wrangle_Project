{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5283456",
   "metadata": {},
   "source": [
    "## Data Wrangle Report\n",
    "\n",
    "### Gathering Data for this Project\n",
    "\n",
    "For this project, data was gathered from the three sources indicated below. Different methods of data collection were utilized for each of the data sources, including:\n",
    "\n",
    "#### Importing data via csv\n",
    "#### Using requests to download data off internet\n",
    "#### Scrape data from an API\n",
    "\n",
    "\n",
    "Three data sources:\n",
    "\n",
    "* Enhanced Twitter Archive - \n",
    "The Udacity-provided WeRateDogs Twitter archive. This only includes some of the basic tweet information for all 5000+ of their tweets. I personally downloaded this file by visiting the `twitter_archive_enhanced.csv` URL.\n",
    "\n",
    "* Image Predictions File - \n",
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (`image_predictions.tsv`) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: image_predictions.tsv\n",
    "\n",
    "* Data via the Twitter API - \n",
    "The number of favorites (or \"likes\") and retweets each tweet has received, as well as any other information you think is relevant. Using the tweet IDs from the WeRateDogs Twitter archive, use Python's Tweepy package to query the Twitter API for each tweet's JSON data, and then store the whole set of JSON information for each tweet in a file called tweet json.txt. JSON data for each tweet should be written to a separate line. Then, each by line, read this.txt file into a pandas DataFrame that contains (at a minimum) the tweet ID, retweets, and favorites.\n",
    "\n",
    "\n",
    "### Data Assessing\n",
    "\n",
    "After gathering the datasets, I then proceed to assess the datasets. Assessment of the dataset was done visually and programmatically.\n",
    "\n",
    "In this section, I focused on the Quality and Tidiness issues of the datasets.\n",
    "\n",
    "At the end of the data assessments, I was able to come up with these issues:\n",
    "\n",
    "#### Quality:\n",
    "\n",
    "##### Enhanced Twitter Archive\n",
    "\n",
    "* The `retweeted_status_timestamp`, `timestamp` data types should be datetime instead of object (string).\n",
    "* Correct numerators with decimals\n",
    "* Some values in `rating_numerator` and `rating_denominator` seem to be in error or suspicious outliers.\n",
    "* Incorrect names or missing names in name column such as, a, an, the... - all are written with lower case letters\n",
    "* Some records have more than one dog stage\n",
    "* Columns not needed for analysis should be dropped.\n",
    "\n",
    "##### Image Predictions File\n",
    "\n",
    "* Since the ID fields, such as `tweet_id` and `in_reply_to_status_id`, aren't numeric and aren't meant to do computations, they should be objects rather than integers or floats.\n",
    "* p1, p2, and p3 should be categorical data type.\n",
    "\n",
    "#### Additional Data via the Twitter API\n",
    "\n",
    "* `Tweet_id` data type is wrong\n",
    "* There are also missing tweets\n",
    "\n",
    "\n",
    "\n",
    "#### Tidiness:\n",
    "\n",
    "##### Enhanced Twitter Archive\n",
    "\n",
    "* Doggo, floofer, pupper, and puppo are the four distinct columns that all relate to the same variable that indicates the stage of the dog. In order to create the column \"`dog_stage`,\" we can combine these columns.\n",
    "\n",
    "##### Additional Data via the Twitter API\n",
    "\n",
    "* The Twitter API table columns (`retweets`, `favorites`, `user_followers`) should be added to the `df_enhanced` dataframe.\n",
    "\n",
    "##### Imgage Predictions File\n",
    "\n",
    "* The `df_image` dataframe should have been added to the `df_enhanced` dataframe.\n",
    "* The 3 dataframes: `df_enhanced`, `df_twt_raw`, and `df_image` should be merged into a single dataframe. \n",
    "\n",
    "\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "To the best of my ability, I was able to overcome the aforementioned problems using my understanding of Python and online resources like Google, Stack Overflow, etc. There was a lot of trial and error involved when using regular expressions in challenging situations, but other tasks, like removing the unwanted columns, removing duplicates were rather simple.\n",
    "\n",
    "Overall, I learned a lot about how to use python effectively and efficiently to clean data and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3da03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
